# Supported Models
---

Llama Assistant supports various AI models to power its capabilities. These models can be run locally on your machine, ensuring privacy and offline functionality.

## Text Models

Llama Assistant currently supports the following text-based language models:

### 🦙 Llama 3.2
- Developed by Meta AI Research
- Sizes: 1B and 3B parameters
- Quantization: 4-bit and 8-bit options available
- Capabilities: General language understanding and generation
- [Model Link](https://github.com/facebookresearch/llama)

### 🐧 Qwen2.5-0.5B-Instruct
- Developed by Alibaba Cloud
- Size: 0.5B parameters  
- Quantization: 4-bit
- Capabilities: Instruction following, task completion
- [Model Link](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF)

## Multimodal Models

For tasks involving both text and images, Llama Assistant supports these multimodal models:

### 🌙 Moondream2
- Developed by Vikhyat Korrapati
- Capabilities: Image understanding, visual question answering
- [Model Link](https://huggingface.co/vikhyatk/moondream2)

### 🖼️ MiniCPM-V-2.6
- Developed by OpenBMB
- Capabilities: Vision-language tasks, image captioning
- [Model Link](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf)

## Using Models

Llama Assistant automatically downloads and manages the appropriate model based on your task and system capabilities. You can specify your preferred model in the settings if desired.

## Future Model Support

We are actively working on expanding our model support. Planned additions include:

- 📚 More text-only models (targeting 5 additional options)
- 🖼️ More multimodal models (targeting 5 additional options)
- 🎙️ Offline speech-to-text models

Check our [GitHub repository](https://github.com/vietanhdev/llama-assistant) for the latest updates on supported models and features.